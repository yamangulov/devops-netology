# Домашнее задание к занятию "6.6. Troubleshooting"

## Задача 1

Перед выполнением задания ознакомьтесь с документацией по [администрированию MongoDB](https://docs.mongodb.com/manual/administration/).

Пользователь (разработчик) написал в канал поддержки, что у него уже 3 минуты происходит CRUD операция в MongoDB и её 
нужно прервать. 

Вы как инженер поддержки решили произвести данную операцию:
- напишите список операций, которые вы будете производить для остановки запроса пользователя

Ответ: необходимо запросить у пользователя строку запроса, который он запустил на выполнение. Затем запускаем команду `db.currentOp()`. В выводе команды будет много полей, нам могут помочь поля query, command и opid. По содержанию двух первых полей можно определить, какая строка относится к подвисшему запросу, сравнив их с запросом, полученным от пользователя. Затем командой `db.killOp(opid)` убиваем нужный процесс, но делать это нужно только в исключительных случаях. 

- предложите вариант решения проблемы с долгими (зависающими) запросами в MongoDB

Ответ: можно изучить список наиболее долгих запросов, например, командой db.currentOp, но она покажет только текущие запросы. Можно также включить мониторинг, например, системный встроенный профайлер командой `db.setProfilingLevel(1)` (тяжелыми будут считаться запросы больше 100 миллисекунд), затем:
находим самые недавние тяжелые запросы 10 штук
`db.system.profile.find().sort({$natural: -1}).limit(10)`
находим топ-10 тяжелых запросов из всех собранных
`db.system.profile.find().sort({millis: -1}).limit(10)`

Затем для этих запросов можно добавить индексы или перестроить существующие, если они давно не обновлялись.

## Задача 2

Перед выполнением задания познакомьтесь с документацией по [Redis latency troobleshooting](https://redis.io/topics/latency).

Вы запустили инстанс Redis для использования совместно с сервисом, который использует механизм TTL. 
Причем отношение количества записанных key-value значений к количеству истёкших значений есть величина постоянная и
увеличивается пропорционально количеству реплик сервиса. 

При масштабировании сервиса до N реплик вы увидели, что:
- сначала рост отношения записанных значений к истекшим
- Redis блокирует операции записи

Как вы думаете, в чем может быть проблема?

Ответ: очевидно, что записывается больше значений, чем освобождается, а по достижении какого-то предела запись блокируется, потому что записанных значений уже слишком много. Redis работает в оперативной памяти, следовательно, проблема показывает как раз недостаток оперативной памяти.
 
## Задача 3

Вы подняли базу данных MySQL для использования в гис-системе. При росте количества записей, в таблицах базы,
пользователи начали жаловаться на ошибки вида:
```python
InterfaceError: (InterfaceError) 2013: Lost connection to MySQL server during query u'SELECT..... '
```

Как вы думаете, почему это начало происходить и как локализовать проблему?

Какие пути решения данной проблемы вы можете предложить?

Когда мы запускаем длинные сложные запросы, которые могут выполняться долго, например, несколько секунд, они пытаются удерживать соединение слишком долго. Чтобы починить, нужно изменить глобальные настройки тайм-аутов сервера БД. Их несколько, мы можем посмотреть их, выполнив запрос `SHOW VARIABLES LIKE "%timeout";` 
Получим что-то вроде такого:
```mysql
+-----------------------------------+----------+
| Variable_name                     | Value    |
+-----------------------------------+----------+
| connect_timeout                   | 10       |
| delayed_insert_timeout            | 300      |
| have_statement_timeout            | YES      |
| innodb_flush_log_at_timeout       | 1        |
| innodb_lock_wait_timeout          | 50       |
| innodb_rollback_on_timeout        | OFF      |
| interactive_timeout               | 28800    |
| lock_wait_timeout                 | 31536000 |
| mysqlx_connect_timeout            | 30       |
| mysqlx_idle_worker_thread_timeout | 60       |
| mysqlx_interactive_timeout        | 28800    |
| mysqlx_port_open_timeout          | 0        |
| mysqlx_read_timeout               | 30       |
| mysqlx_wait_timeout               | 28800    |
| mysqlx_write_timeout              | 60       |
| net_read_timeout                  | 30       |
| net_write_timeout                 | 60       |
| replica_net_timeout               | 60       |
| rpl_stop_replica_timeout          | 31536000 |
| rpl_stop_slave_timeout            | 31536000 |
| slave_net_timeout                 | 60       |
| wait_timeout                      | 28800    |
+-----------------------------------+----------+
```

После чего можно оценить существующие таймауты и увеличить какие-нибудь из них. Например, непосредственно выполнить запрос такого вида:

`SET GLOBAL connect_timeout = 600; `

можно поэкспериментировать с разными, потом записать в конфигурацию my.cnf самые удачные, чтобы они сохранялись при рестарте сервера. Например, самые влияющие на потерю соединения чаще все эти:

```mysql
[mysqld]
connect_timeout = 10
net_read_timeout = 30
```
Можно также попробовать увеличивать max_connections в сочетании с этим настройками.

## Задача 4


Вы решили перевести гис-систему из задачи 3 на PostgreSQL, так как прочитали в документации, что эта СУБД работает с 
большим объемом данных лучше, чем MySQL.

После запуска пользователи начали жаловаться, что СУБД время от времени становится недоступной. В dmesg вы видите, что:

`postmaster invoked oom-killer`

Как вы думаете, что происходит?

Как бы вы решили данную проблему?

Ответ: out-of-memory killer - это специальный процесс, который выбрает способ завершить проблемный процесс, который съедает слишком много оперативки, вместо того, чтобы дождаться падения ядра, и чтобы наоборот - предотвратить такое падение. Очевидно, что в системен не хватает ОП. Необходимо подобрать размер ОП, которого достаточно для задач Postgresql + размер ОП, которого достаточно для остальных системных задач. После чего необходимо 1) если не хватает на сумму всех задач все равно, увеличить общий объем ОП 2) независимо от выполнения п.1 необходимо настроить в posgresql.conf максимальный размер shared_buffers в 40 процентов от ОП сервера
